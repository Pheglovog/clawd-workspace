# LangChain é«˜çº§ç‰¹æ€§ - VectorStore å’Œ Callbacks

## ğŸ“‹ æ¦‚è¿°

LangChain æä¾›äº†è®¸å¤šé«˜çº§ç‰¹æ€§ï¼ŒåŒ…æ‹¬å‘é‡å­˜å‚¨ã€å›è°ƒå‡½æ•°ã€ç´¢å¼•ç­‰ï¼Œç”¨äºæ„å»ºæ›´å¼ºå¤§çš„ AI åº”ç”¨ã€‚

## ğŸ§  VectorStoreï¼ˆå‘é‡å­˜å‚¨ï¼‰

### åŸºæœ¬æ¦‚å¿µ

å‘é‡å­˜å‚¨ç”¨äºå­˜å‚¨å’Œæ£€ç´¢æ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼Œå®ç°è¯­ä¹‰æœç´¢å’Œ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ã€‚

### æ”¯æŒçš„å‘é‡æ•°æ®åº“

| æ•°æ®åº“ | å®‰è£… | ç‰¹ç‚¹ |
|-------|------|------|
| ChromaDB | `pip install chromadb` | è½»é‡ï¼Œæœ¬åœ°è¿è¡Œ |
| FAISS | `pip install faiss-cpu` | é«˜æ€§èƒ½ï¼ŒMeta å¼€æº |
| Pinecone | `pip install pinecone-client` | äº‘ç«¯ï¼Œå¯æ‰©å±• |
| Weaviate | `pip install weaviate-client` | å¼€æºï¼Œæ”¯æŒè¿‡æ»¤ |
| Qdrant | `pip install qdrant-client` | é«˜æ€§èƒ½ï¼Œæ˜“äºéƒ¨ç½² |
| Milvus | `pip install pymilvus` | å¼€æºï¼Œä¼ä¸šçº§ |

### ChromaDB ç¤ºä¾‹

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embeddings = OpenAIEmbeddings()

# ç¤ºä¾‹æ–‡æœ¬
texts = [
    "Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€",
    "LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ LLM åº”ç”¨çš„æ¡†æ¶",
    "å‘é‡å­˜å‚¨ç”¨äºå®ç°è¯­ä¹‰æœç´¢",
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯"
]

# åˆ›å»ºå‘é‡å­˜å‚¨
vectorstore = Chroma.from_texts(texts, embeddings)

# ç›¸ä¼¼åº¦æœç´¢
query = "ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ"
results = vectorstore.similarity_search(query, k=2)

for i, doc in enumerate(results):
    print(f"Result {i+1}: {doc.page_content}")
    print(f"Score: {doc.metadata.get('score', 'N/A')}")
    print()
```

### æŒä¹…åŒ–å‘é‡å­˜å‚¨

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# åˆ›å»ºå¹¶æŒä¹…åŒ–
vectorstore = Chroma.from_documents(
    documents,
    OpenAIEmbeddings(),
    persist_directory="./chroma_db"
)

# åŠ è½½å·²å­˜å‚¨çš„å‘é‡æ•°æ®åº“
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=OpenAIEmbeddings()
)

# æœç´¢
results = vectorstore.similarity_search("æŸ¥è¯¢å†…å®¹")
```

### æ–‡æ¡£åŠ è½½å’Œåˆ†å—

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åŠ è½½æ–‡æ¡£
loader = TextLoader("large_document.txt")
documents = loader.load()

# åˆ†å—
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # æ¯å— 1000 å­—ç¬¦
    chunk_overlap=200,    # é‡å  200 å­—ç¬¦
    length_function=len
)

splits = text_splitter.split_documents(documents)

# åˆ›å»ºå‘é‡å­˜å‚¨
vectorstore = Chroma.from_documents(splits, OpenAIEmbeddings())
```

### ç›¸ä¼¼åº¦æœç´¢ç±»å‹

```python
from langchain.vectorstores import Chroma

# 1. ç›¸ä¼¼åº¦æœç´¢
results = vectorstore.similarity_search(query, k=3)

# 2. ç›¸ä¼¼åº¦æœç´¢ï¼ˆå¸¦åˆ†æ•°ï¼‰
results = vectorstore.similarity_search_with_score(query, k=3)
for doc, score in results:
    print(f"Score: {score:.4f} - {doc.page_content}")

# 3. æœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼ˆMMRï¼‰æœç´¢
# è¿”å›å¤šæ ·åŒ–ç»“æœï¼Œé¿å…é‡å¤
results = vectorstore.max_marginal_relevance_search(
    query,
    k=3,
    fetch_k=10  # ä»å‰ 10 ä¸ªç»“æœä¸­é€‰æ‹© 3 ä¸ªæœ€å¤šæ ·åŒ–çš„
)

# 4. æŒ‰åˆ†æ•°è¿‡æ»¤
results = vectorstore.similarity_search_with_relevance_scores(
    query,
    score_threshold=0.7  # åªè¿”å›åˆ†æ•° >= 0.7 çš„ç»“æœ
)
```

### å…ƒæ•°æ®è¿‡æ»¤

```python
# åˆ›å»ºå¸¦å…ƒæ•°æ®çš„æ–‡æ¡£
documents = [
    Document(page_content="Python åŸºç¡€æ•™ç¨‹", metadata={"category": "python", "level": "beginner"}),
    Document(page_content="Python é«˜çº§æŠ€å·§", metadata={"category": "python", "level": "advanced"}),
    Document(page_content="JavaScript åŸºç¡€", metadata={"category": "javascript", "level": "beginner"}),
]

vectorstore = Chroma.from_documents(documents, embeddings)

# è¿‡æ»¤æœç´¢
results = vectorstore.similarity_search(
    "ç¼–ç¨‹æ•™ç¨‹",
    k=2,
    filter={"category": "python"}
)
```

### FAISS ç¤ºä¾‹

```python
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

# åˆ›å»º FAISS ç´¢å¼•
vectorstore = FAISS.from_documents(documents, embeddings)

# ä¿å­˜ç´¢å¼•
vectorstore.save_local("faiss_index")

# åŠ è½½ç´¢å¼•
vectorstore = FAISS.load_local("faiss_index", embeddings)

# æœç´¢
results = vectorstore.similarity_search(query)
```

### Pinecone ç¤ºä¾‹

```python
import pinecone
from langchain.vectorstores import Pinecone
from langchain.embeddings import OpenAIEmbeddings

# åˆå§‹åŒ– Pinecone
pinecone.init(
    api_key="your-api-key",
    environment="us-east-1-aws"
)

# åˆ›å»ºç´¢å¼•
index_name = "my-index"
if index_name not in pinecone.list_indexes():
    pinecone.create_index(
        name=index_name,
        dimension=1536,  # OpenAI åµŒå…¥ç»´åº¦
        metric="cosine"
    )

# åˆ›å»ºå‘é‡å­˜å‚¨
vectorstore = Pinecone.from_documents(
    documents,
    OpenAIEmbeddings(),
    index_name=index_name
)

# æœç´¢
results = vectorstore.similarity_search(query, k=5)
```

## ğŸ”” Callbacksï¼ˆå›è°ƒå‡½æ•°ï¼‰

### åŸºæœ¬æ¦‚å¿µ

å›è°ƒå‡½æ•°ç”¨äºåœ¨ LangChain æ‰§è¡Œè¿‡ç¨‹ä¸­ç›‘å¬å’Œå¤„ç†äº‹ä»¶ï¼Œå¦‚æ—¥å¿—è®°å½•ã€æ€§èƒ½ç›‘æ§ã€é”™è¯¯å¤„ç†ç­‰ã€‚

### å†…ç½®å›è°ƒå¤„ç†å™¨

```python
from langchain.callbacks import StdOutCallbackHandler

# æ ‡å‡†è¾“å‡ºå›è°ƒ
handler = StdOutCallbackHandler()

# ä½¿ç”¨å›è°ƒ
chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])
chain.run("Hello")
```

### è‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨

```python
from langchain.callbacks.base import BaseCallbackHandler
from typing import Any, Dict, List, Optional

class MyCustomHandler(BaseCallbackHandler):
    def __init__(self):
        self.token_count = 0
        self.start_time = None

    def on_llm_start(
        self,
        serialized: Dict[str, Any],
        prompts: List[str],
        **kwargs: Any
    ) -> Any:
        """å½“ LLM å¼€å§‹ç”Ÿæˆæ—¶è°ƒç”¨"""
        print(f"\n{'='*50}")
        print(f"LLM Start - Prompts: {prompts}")
        self.start_time = time.time()

    def on_llm_new_token(
        self,
        token: str,
        **kwargs: Any
    ) -> Any:
        """æ¯å½“ç”Ÿæˆæ–° token æ—¶è°ƒç”¨"""
        self.token_count += 1
        print(token, end="", flush=True)

    def on_llm_end(
        self,
        response: Any,
        **kwargs: Any
    ) -> Any:
        """å½“ LLM å®Œæˆç”Ÿæˆæ—¶è°ƒç”¨"""
        duration = time.time() - self.start_time
        print(f"\n{'='*50}")
        print(f"LLM End - Tokens: {self.token_count}, Time: {duration:.2f}s")
        print(f"Speed: {self.token_count / duration:.2f} tokens/s")

    def on_chain_start(
        self,
        serialized: Dict[str, Any],
        inputs: Dict[str, Any],
        **kwargs: Any
    ) -> Any:
        """å½“é“¾å¼€å§‹æ‰§è¡Œæ—¶è°ƒç”¨"""
        print(f"\n[Chain Start] {serialized.get('id', ['unknown'])[0]}")

    def on_chain_end(
        self,
        outputs: Dict[str, Any],
        **kwargs: Any
    ) -> Any:
        """å½“é“¾å®Œæˆæ‰§è¡Œæ—¶è°ƒç”¨"""
        print(f"[Chain End] Output: {outputs}")

    def on_chain_error(
        self,
        error: Exception,
        **kwargs: Any
    ) -> Any:
        """å½“é“¾æ‰§è¡Œå‡ºé”™æ—¶è°ƒç”¨"""
        print(f"[Chain Error] {error}")

    def on_tool_start(
        self,
        serialized: Dict[str, Any],
        input_str: str,
        **kwargs: Any
    ) -> Any:
        """å½“å·¥å…·å¼€å§‹æ‰§è¡Œæ—¶è°ƒç”¨"""
        print(f"[Tool Start] {serialized['name']}: {input_str}")

    def on_tool_end(
        self,
        output: str,
        **kwargs: Any
    ) -> Any:
        """å½“å·¥å…·å®Œæˆæ‰§è¡Œæ—¶è°ƒç”¨"""
        print(f"[Tool End] Result: {output}")

    def on_agent_action(
        self,
        action: Any,
        **kwargs: Any
    ) -> Any:
        """å½“ Agent æ‰§è¡ŒåŠ¨ä½œæ—¶è°ƒç”¨"""
        print(f"[Agent Action] {action.tool}: {action.tool_input}")
```

### ä½¿ç”¨è‡ªå®šä¹‰å›è°ƒ

```python
from langchain.chat_models import ChatOpenAI
from langchain.agents import initialize_agent, Tool, AgentType

# åˆå§‹åŒ–
handler = MyCustomHandler()
llm = ChatOpenAI(temperature=0, callbacks=[handler])

# åˆ›å»º Agent
tools = [
    Tool(
        name="Calculator",
        func=lambda x: str(eval(x)),
        description="Useful for math calculations"
    )
]

agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
    verbose=True
)

# è¿è¡Œ Agent
agent.run("What is 123 * 456?")
```

### LangSmith å›è°ƒ

```python
from langchain.callbacks.tracers import LangChainTracer
from langchain.smith import LangSmith

# åˆå§‹åŒ– LangSmith
tracer = LangChainTracer(
    project_name="my-project",
    client=LangSmith(
        api_key="your-api-key",
        api_url="https://api.smith.langchain.com"
    )
)

# ä½¿ç”¨å›è°ƒ
chain = LLMChain(llm=llm, prompt=prompt, callbacks=[tracer])
result = chain.run("Hello")

# æŸ¥çœ‹æ‰§è¡Œè¯¦æƒ…åœ¨ https://smith.langchain.com
```

### æµå¼è¾“å‡ºå›è°ƒ

```python
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# æµå¼è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡º
llm = ChatOpenAI(
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]
)

result = llm.predict("Tell me a story")
```

### è‡ªå®šä¹‰æµå¼å›è°ƒ

```python
from typing import Any, Dict
from langchain.callbacks.base import BaseCallbackHandler

class TokenCollector(BaseCallbackHandler):
    def __init__(self):
        self.tokens = []

    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        self.tokens.append(token)

# ä½¿ç”¨
collector = TokenCollector()
llm = ChatOpenAI(streaming=True, callbacks=[collector])

result = llm.predict("Hello")
print(f"All tokens: {collector.tokens}")
```

### ç»„åˆå¤šä¸ªå›è°ƒ

```python
from langchain.callbacks import get_openai_callback

# OpenAI è°ƒç”¨ç»Ÿè®¡å›è°ƒ
with get_openai_callback() as cb:
    result = chain.run("Hello")

    print(f"Total Tokens: {cb.total_tokens}")
    print(f"Prompt Tokens: {cb.prompt_tokens}")
    print(f"Completion Tokens: {cb.completion_tokens}")
    print(f"Total Cost (USD): ${cb.total_cost:.4f}")
```

### å¼‚æ­¥å›è°ƒ

```python
from langchain.callbacks.base import AsyncCallbackHandler

class AsyncTokenHandler(AsyncCallbackHandler):
    async def on_llm_new_token(self, token: str, **kwargs) -> Any:
        # å¼‚æ­¥å¤„ç† token
        await self.send_to_websocket(token)

    async def send_to_websocket(self, token: str):
        # å‘é€åˆ° WebSocket
        pass

# ä½¿ç”¨
handler = AsyncTokenHandler()
llm = ChatOpenAI(streaming=True, callbacks=[handler])
result = await llm.apredict("Hello")
```

## ğŸ¯ å®é™…åº”ç”¨ç¤ºä¾‹

### RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# 1. åŠ è½½æ–‡æ¡£
loader = TextLoader("knowledge_base.txt")
documents = loader.load()

# 2. åˆ†å—
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(documents)

# 3. åˆ›å»ºå‘é‡å­˜å‚¨
vectorstore = Chroma.from_documents(splits, OpenAIEmbeddings())
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 4. åˆ›å»º RAG é“¾
llm = ChatOpenAI(temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# 5. æŸ¥è¯¢
query = "ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ"
result = qa_chain({"query": query})

print(f"Answer: {result['result']}")
print(f"\nSources:")
for doc in result["source_documents"]:
    print(f"- {doc.page_content[:100]}...")
```

### å¸¦å›è°ƒçš„ RAG

```python
from langchain.callbacks.base import BaseCallbackHandler

class RAGCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        self.retrieved_docs = []
        self.generated_tokens = []

    def on_retriever_start(self, query: str, **kwargs):
        print(f"\nğŸ” Retrieving documents for: {query}")

    def on_retriever_end(self, documents, **kwargs):
        self.retrieved_docs = documents
        print(f"ğŸ“š Retrieved {len(documents)} documents")

    def on_llm_new_token(self, token: str, **kwargs):
        self.generated_tokens.append(token)
        print(token, end="", flush=True)

# ä½¿ç”¨
handler = RAGCallbackHandler()
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(streaming=True, callbacks=[handler]),
    chain_type="stuff",
    retriever=retriever
)

result = qa_chain.run("æŸ¥è¯¢å†…å®¹")
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

```python
from langchain.callbacks import get_openai_callback
import time

# ç›‘æ§æ€§èƒ½å’Œæˆæœ¬
with get_openai_callback() as cb:
    start_time = time.time()

    result = chain.run("å¤æ‚æŸ¥è¯¢")

    duration = time.time() - start_time

    print(f"\n{'='*40}")
    print(f"Performance Report")
    print(f"{'='*40}")
    print(f"Duration: {duration:.2f}s")
    print(f"Total Tokens: {cb.total_tokens}")
    print(f"Prompt Tokens: {cb.prompt_tokens}")
    print(f"Completion Tokens: {cb.completion_tokens}")
    print(f"Total Cost: ${cb.total_cost:.4f}")
    print(f"Tokens/Second: {cb.total_tokens/duration:.2f}")
```

---

**æ›´æ–°æ—¶é—´**: 2026-02-03
